{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJMHAKI8YuvAjN1Gg+jEkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerwzeng/BigDataSystems/blob/main/mu_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\mu$-Two Implementation\n",
        "\n",
        "### Harvard University, Spring 2025\n",
        "### CS265 Big Data Systems - Term Project (Systems)  \n",
        "#### Roger W. Zeng\n"
      ],
      "metadata": {
        "id": "--GDyyxobHoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.fx as fx\n",
        "from torch.fx import Interpreter, GraphModule, symbolic_trace\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import torch.cuda as cuda\n",
        "import torch.cuda.nvtx as nvtx\n",
        "import torchvision.models as models\n",
        "from typing import Dict, Set, List, Any\n",
        "import operator\n"
      ],
      "metadata": {
        "id": "xZtqkjuiIEa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Nodes in the computation graph\n",
        "#   Profiling Attributes\n",
        "#   rank: The position of the node in the topological sort of the graph.\n",
        "#   gtype: The type of graph this node belongs to, either forward or backward pass.\n",
        "#   run_time: The runtime of the node in milliseconds.\n",
        "#   peak_mem: The peak memory usage in bytes.\n",
        "#   active_mem: The active memory usage in bytes, representing the minimum required memory.\n",
        "#   Scheduling Attributes\n",
        "#   to_offload: A list of nodes to be offloaded to host memory after this node is executed.\n",
        "#   to_delete: A list of nodes to be deleted after this node is executed, further aiding in memory optimization.\n",
        "#   to_prefetch: A list of nodes to be prefetched from the host memory before this node is executed.\n",
        "#   to_recompute: A list of nodes to be recomputed before this node is executed. This is relevant for activation checkpointing.\n",
        "# ------------------------------\n",
        "\"\"\"\n",
        "class Node:\n",
        "    def __init__(self, rank, gtype, func=None):\n",
        "        self.func = func\n",
        "\"\"\"\n",
        "\n",
        "class Node(fx.Node):  # Inherit from torch.fx.Node\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)  # Initialize parent class attributes\n",
        "        self.rank = 0\n",
        "        self.gtype = 'forward'\n",
        "        self.run_time = None\n",
        "        self.peak_mem = None\n",
        "        self.active_mem = None\n",
        "        self.to_offload = []\n",
        "        self.to_delete = []\n",
        "        self.to_prefetch = []\n",
        "        self.to_recompute = []\n",
        "\n",
        "    def execute(self, *args, **kwargs):\n",
        "        if self.func:\n",
        "            start_time = time.time()\n",
        "            result = self.func(*args, **kwargs)\n",
        "            end_time = time.time()\n",
        "            self.run_time = (end_time - start_time) * 1000  # in milliseconds\n",
        "            self.peak_mem = torch.cuda.max_memory_allocated()  # in bytes\n",
        "            self.active_mem = torch.cuda.memory_allocated()  # in bytes\n",
        "            return result\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Intermediate nodes (feature map tensors) with\n",
        "# profiling, Swapping and Recomputation attributes\n",
        "#\n",
        "# initialized with a tensor and optional references to the last forward access, first backward access, and last backward access nodes.\n",
        "# -----------------------------------------------------\n",
        "class IntermediateNode:\n",
        "    def __init__(self, tensor, last_fw_access=None, first_bw_access=None, last_bw_access=None):\n",
        "        self.tensor = tensor\n",
        "        self.memory_size = tensor.storage().nbytes()\n",
        "        self.inactive_time = None\n",
        "        self.swap_time = None\n",
        "        self.last_fw_access = last_fw_access\n",
        "        self.first_bw_access = first_bw_access\n",
        "        self.last_bw_access = last_bw_access\n",
        "        self.prefetch_prompt = None\n",
        "        self.active_fw_interval = None\n",
        "        self.active_bw_interval = None\n",
        "        self.recomp_srcs = []\n",
        "        self.recomp_graph = None\n",
        "        self.recomp_cnt = 0\n",
        "        self.recomp_time = None\n",
        "        self.total_recomp_time = 0.0\n",
        "        self.recomp_memory = None\n",
        "        self.recompute_ratio = None\n",
        "\n",
        "    def update_inactive_time(self, start_time, end_time):\n",
        "        \"\"\"Update the inactive time of the tensor.\"\"\"\n",
        "        self.inactive_time = end_time - start_time\n",
        "\n",
        "    def update_swap_time(self, swap_time):\n",
        "        \"\"\"Update the swap time of the tensor.\"\"\"\n",
        "        self.swap_time = swap_time\n",
        "\n",
        "    def update_recomputation(self, recomputation_time, recomputation_memory):\n",
        "        \"\"\"Update recomputation attributes.\"\"\"\n",
        "        self.recomp_time = recomputation_time\n",
        "        self.recomp_memory = recomputation_memory\n",
        "        self.recompute_ratio = self.memory_size / self.recomp_time if self.recomp_time else 0\n",
        "\n",
        "    def mark_for_recomputation(self, recomputation_sources):\n",
        "        \"\"\"Mark the node for recomputation and set its sources.\"\"\"\n",
        "        self.recomp_srcs = recomputation_sources\n",
        "\n",
        "    def set_prefetch_prompt(self, prompt_node):\n",
        "        \"\"\"Set the prefetch prompt node.\"\"\"\n",
        "        self.prefetch_prompt = prompt_node\n",
        "\n",
        "    def set_active_intervals(self, fw_start_node, fw_end_node, bw_start_node, bw_end_node):\n",
        "        \"\"\"Set the active intervals for forward and backward passes.\"\"\"\n",
        "        self.active_fw_interval = (fw_start_node, fw_end_node)\n",
        "        self.active_bw_interval = (bw_start_node, bw_end_node)\n",
        "\n",
        "    def recomp_sub_graph(input_tensor):\n",
        "        # Example recomputation: linear transformation followed by ReLU\n",
        "        linear_layer = nn.Linear()  #in_features=..., out_features=...)\n",
        "        relu = nn.ReLU()\n",
        "        output_tensor = relu(linear_layer(input_tensor))\n",
        "        return output_tensor\n",
        "\n",
        "    def recompute_tensor(self):\n",
        "        \"\"\"Simulate recomputation of the tensor.\"\"\"\n",
        "        # Placeholder for recomputation logic\n",
        "        if self.recomp_graph and self.recomp_srcs:\n",
        "            # Execute the sub-graph to regenerate the tensor\n",
        "            self.tensor = self.recomp_sub_graph(self.recomp_srcs)\n",
        "            self.recomp_cnt += 1\n",
        "            self.total_recomp_time += self.recomp_time\n"
      ],
      "metadata": {
        "id": "UX8Qvw99cWAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "class MuTwoProfiler(fx.Interpreter):\n",
        "  def __init__(self, graph_module: fx.GraphModule, garbage_collect_values=False, graph=None):\n",
        "    super().__init__(graph_module, garbage_collect_values, graph)\n",
        "    self.profiler = profile(activities=[ProfilerActivity.CUDA], record_shapes=True)\n",
        "    self.compute_nodes: Dict[str, Node] = {}  # Stores compute nodes (forward/backward ops)\n",
        "    self.intermediate_nodes: Dict[str, IntermediateNode] = {}  # Stores tensor nodes\n",
        "\n",
        "    # Initialize pinned memory buffer and tracking\n",
        "    self.buffer_size = 1024 * 1024 * 1024  # 1GB buffer\n",
        "    self.pinned_buffer = torch.empty(self.buffer_size, pin_memory=True)\n",
        "    self.buffer_offset = 0\n",
        "    self.tensor_buffer_map = {}  # Maps tensor_id -> (offset, size)\n",
        "\n",
        "    # Warm up CUDA caching allocator\n",
        "    self._warmup_cuda_allocator()\n",
        "\n",
        "  def _warmup_cuda_allocator(self):\n",
        "    \"\"\"Warm up the CUDA caching allocator to stabilize measurements\"\"\"\n",
        "    dummy = torch.empty(1024 * 1024, device='cuda')  # 1MB tensor\n",
        "    del dummy\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  def _allocate_buffer_space(self, tensor_size: int) -> int:\n",
        "    \"\"\"Allocate space in pinned buffer for tensor\"\"\"\n",
        "    if self.buffer_offset + tensor_size > self.buffer_size:\n",
        "        self.buffer_offset = 0  # Reset if full (could be more sophisticated)\n",
        "\n",
        "    offset = self.buffer_offset\n",
        "    self.buffer_offset += tensor_size\n",
        "    return offset\n",
        "\n",
        "  def run_node(self, node: fx.Node) -> Any:\n",
        "    with record_function(str(node)):\n",
        "        with self.profiler:\n",
        "          torch.cuda.reset_peak_memory_stats()\n",
        "          result = super().run_node(node)\n",
        "          memory_usage = torch.cuda.max_memory_allocated()\n",
        "    return result\n",
        "\n",
        "\n",
        "  def _should_swap_out(self, intermediate_node: IntermediateNode) -> bool:\n",
        "    \"\"\"Determine if a tensor should be swapped out based on access patterns\"\"\"\n",
        "    if not intermediate_node.last_fw_access:\n",
        "        return False\n",
        "\n",
        "    # Check if tensor won't be needed soon and is large enough to be worth swapping\n",
        "    memory_threshold = 1024 * 1024  # 1MB\n",
        "    return (intermediate_node.memory_size > memory_threshold and\n",
        "            not intermediate_node.first_bw_access)\n",
        "\n",
        "  def _measure_swap_time(self, tensor: torch.Tensor, operation: str) -> float:\n",
        "    \"\"\"Measure time for swapping tensors between CPU and GPU using pinned memory\"\"\"\n",
        "    start_event = cuda.Event(enable_timing=True)\n",
        "    end_event = cuda.Event(enable_timing=True)\n",
        "\n",
        "    tensor_id = id(tensor)\n",
        "    tensor_size = tensor.nelement() * tensor.element_size()\n",
        "\n",
        "    start_event.record()\n",
        "\n",
        "    if operation == \"swap_out\":\n",
        "        # Allocate buffer space if not already allocated\n",
        "        if tensor_id not in self.tensor_buffer_map:\n",
        "            offset = self._allocate_buffer_space(tensor_size)\n",
        "            self.tensor_buffer_map[tensor_id] = (offset, tensor_size)\n",
        "\n",
        "        # Copy tensor to pinned memory buffer\n",
        "        offset, _ = self.tensor_buffer_map[tensor_id]\n",
        "        buffer_view = self.pinned_buffer[offset:offset + tensor_size].view_as(tensor)\n",
        "        buffer_view.copy_(tensor.cpu())\n",
        "        tensor.storage().resize_(0)  # Free GPU memory\n",
        "\n",
        "    elif operation == \"swap_in\":\n",
        "      if tensor_id in self.tensor_buffer_map:\n",
        "        offset, size = self.tensor_buffer_map[tensor_id]\n",
        "        # Resize tensor and copy from buffer to GPU\n",
        "        tensor.storage().resize_(size // tensor.element_size())\n",
        "        buffer_view = self.pinned_buffer[offset:offset + size].view_as(tensor)\n",
        "        tensor.copy_(buffer_view.cuda())\n",
        "\n",
        "    end_event.record()\n",
        "    end_event.synchronize()\n",
        "    return start_event.elapsed_time(end_event)\n",
        "\n",
        "  def _get_memory_stats(self) -> Dict:\n",
        "    \"\"\"Get current GPU memory statistics\"\"\"\n",
        "    return {\n",
        "      'allocated': torch.cuda.memory_allocated(),\n",
        "      'reserved': torch.cuda.memory_reserved(),\n",
        "      'max_allocated': torch.cuda.max_memory_allocated()\n",
        "    }\n",
        "\n",
        "  def analyze_recomputation(self, node: fx.Node):\n",
        "    \"\"\"Analyze potential recomputation opportunities\"\"\"\n",
        "    if node.name in self.intermediate_nodes:\n",
        "        intermediate_node = self.intermediate_nodes[node.name]\n",
        "\n",
        "        # Simulate recomputation to measure time and memory impact\n",
        "        with profile(activities=[ProfilerActivity.CUDA]) as prof:\n",
        "            intermediate_node.recompute_tensor()\n",
        "\n",
        "        events = prof.key_averages()\n",
        "        recomp_time = events.total_average.cuda_time_ns / 1e6  # Convert to ms\n",
        "        recomp_memory = torch.cuda.max_memory_allocated()\n",
        "\n",
        "        intermediate_node.update_recomputation(recomp_time, recomp_memory)\n",
        "\n",
        "  def get_profiling_results(self) -> Dict:\n",
        "    \"\"\"Return collected profiling statistics\"\"\"\n",
        "    return {\n",
        "        'compute_nodes': self.compute_nodes,\n",
        "        'intermediate_nodes': self.intermediate_nodes\n",
        "    }\n",
        "\n",
        "  def _track_tensor_usage(self, node: fx.Node):\n",
        "    \"\"\"Track tensor usage patterns for each node\"\"\"\n",
        "    for input_node in node.all_input_nodes:\n",
        "        if input_node not in self.tensor_usage:\n",
        "            self.tensor_usage[input_node] = {'first_use': node, 'last_use': node}\n",
        "        else:\n",
        "            self.tensor_usage[input_node]['last_use'] = node\n",
        "\n",
        "  def _get_backward_tensors(self, node: fx.Node) -> List[torch.Tensor]:\n",
        "    \"\"\"Get tensors needed for backward pass\"\"\"\n",
        "    # Implementation depends on specific backward pass requirements\n",
        "    return []\n",
        "\n",
        "  def _get_forward_tensors(self, node: fx.Node) -> List[torch.Tensor]:\n",
        "    \"\"\"Get tensors from forward pass that might be candidates for swapping\"\"\"\n",
        "    # Implementation depends on specific forward pass requirements\n",
        "    return []\n",
        "\n",
        "  def _is_last_use(self, node: fx.Node, tensor: torch.Tensor) -> bool:\n",
        "    \"\"\"Check if this is the last use of a tensor\"\"\"\n",
        "    tensor_node = self._find_tensor_node(tensor)\n",
        "    return (tensor_node in self.tensor_usage and\n",
        "            self.tensor_usage[tensor_node]['last_use'] == node)\n",
        "\n",
        "  def _find_tensor_node(self, tensor: torch.Tensor) -> fx.Node:\n",
        "    \"\"\"Find the node that produced this tensor\"\"\"\n",
        "    # Implementation depends on how tensors are tracked in the graph\n",
        "    return None\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Cl4PanYbAq2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save this for later\n",
        "\"\"\"\n",
        "  # Profile node execution\n",
        "  def run_node(self, node: fx.Node) -> Any:\n",
        "    # Create or get compute node\n",
        "    if node.name not in self.compute_nodes:\n",
        "      self.compute_nodes[node.name] = Node(\n",
        "          rank=len(self.compute_nodes),\n",
        "          gtype='forward' if not node.name.startswith('backward') else 'backward',\n",
        "          func=node.target\n",
        "      )\n",
        "\n",
        "    compute_node = self.compute_nodes[node.name]\n",
        "\n",
        "    # Handle swap-ins for required tensors\n",
        "    for arg in node.args:\n",
        "      if isinstance(arg, fx.Node) and arg.name in self.intermediate_nodes:\n",
        "        tensor_node = self.intermediate_nodes[arg.name]\n",
        "        if tensor_node.last_fw_access and not tensor_node.first_bw_access:\n",
        "          swap_time = self._measure_swap_time(tensor_node.tensor, \"swap_in\")\n",
        "          tensor_node.update_swap_time(swap_time)\n",
        "\n",
        "    # Execute and profile the node\n",
        "    result = compute_node.execute(*node.args, **node.kwargs)\n",
        "\n",
        "    # Create IntermediateNode for output tensor if applicable\n",
        "    if isinstance(result, torch.Tensor):\n",
        "      tensor_name = f\"{node.name}_output\"\n",
        "      if tensor_name not in self.intermediate_nodes:\n",
        "        self.intermediate_nodes[tensor_name] = IntermediateNode(\n",
        "            tensor=result,\n",
        "            last_fw_access=compute_node\n",
        "          )\n",
        "\n",
        "      intermediate_node = self.intermediate_nodes[tensor_name]\n",
        "\n",
        "      # Check if tensor can be swapped out\n",
        "      if self._should_swap_out(intermediate_node):\n",
        "        swap_time = self._measure_swap_time(result, \"swap_out\")\n",
        "        intermediate_node.update_swap_time(swap_time)\n",
        "\n",
        "    return result\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "ieldeSL0AWFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleModel, self).__init__()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = x + 2\n",
        "    #logits = x * 3\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "  def modify_model(graph, model):\n",
        "    for node in graph.nodes:\n",
        "        if node.op == 'call_module':\n",
        "            try:\n",
        "                # Split the target to handle nested modules\n",
        "                target_parts = node.target.split('.')\n",
        "                current_module = model\n",
        "                for part in target_parts:\n",
        "                    current_module = getattr(current_module, part)\n",
        "\n",
        "                # Check and replace ReLU with Sigmoid\n",
        "                if isinstance(current_module, nn.ReLU):\n",
        "                    print(\"modified ReLU to Sigmoid\")\n",
        "                    # Replace ReLU with Sigmoid using the parent module and child name\n",
        "                    parent_module = model\n",
        "                    for part in target_parts[:-1]:  # Access parent module\n",
        "                        parent_module = getattr(parent_module, part)\n",
        "                    setattr(parent_module, target_parts[-1], nn.Sigmoid())\n",
        "\n",
        "            except AttributeError:\n",
        "                pass  # Ignore if attribute not found\n",
        "    return model\n",
        "\n",
        "    \"\"\"\n",
        "              pass  # Ignore if attribute not found\n",
        "\n",
        "      if node.op == 'call_module' and type(getattr(model, node.target)) is nn.ReLU:\n",
        "        print(\"modified ReLU to Sigmoid\")\n",
        "        # Replace ReLU with Sigmoid\n",
        "        setattr(model, node.target, nn.Sigmoid())\n",
        "\n",
        "      if node.op == \"call_function\" and node.target == operator.mul:\n",
        "        print(\"changed fro mul to add\")\n",
        "        node.target = operator.add\n",
        "    return graph\n",
        "    \"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # instantiate the model\n",
        "  model = SimpleModel()\n",
        "\n",
        "  # symbolic trace\n",
        "  symbolic_traced: fx.GraphModule = symbolic_trace(model)\n",
        "  print(f\"Symbolic Traced Graph:{symbolic_traced.graph}\")\n",
        "  print(f\"Symbolic Traced Code:{symbolic_traced.code}\")\n",
        "\n",
        "  # Modifiy the graph\n",
        "  print(f\"Output before modification:{model(torch.rand(1, 28*28))}\")\n",
        "  #print(f\"Output before modification:{model(torch.tensor(3.0))}\")\n",
        "\n",
        "  modified_model = SimpleModel.modify_model(symbolic_traced.graph, model)\n",
        "  modified_traced: fx.GraphModule = symbolic_trace(modified_model)\n",
        "  print(f\"Traced Modified Graph:{modified_traced.graph}\")\n",
        "  #print(f\"Output after modification:{modified_model(torch.tensor(3.0))}\")\n",
        "  print(f\"Output after modification:{modified_model(torch.rand(1, 28*28))}\")\n"
      ],
      "metadata": {
        "id": "3ABAo7f_A0aV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c5cd1cd-0d01-4515-f8be-03d393ea8a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symbolic Traced Graph:graph():\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %linear_relu_stack_0 : [num_users=1] = call_module[target=linear_relu_stack.0](args = (%x,), kwargs = {})\n",
            "    %linear_relu_stack_1 : [num_users=1] = call_module[target=linear_relu_stack.1](args = (%linear_relu_stack_0,), kwargs = {})\n",
            "    %linear_relu_stack_2 : [num_users=1] = call_module[target=linear_relu_stack.2](args = (%linear_relu_stack_1,), kwargs = {})\n",
            "    %linear_relu_stack_3 : [num_users=1] = call_module[target=linear_relu_stack.3](args = (%linear_relu_stack_2,), kwargs = {})\n",
            "    %linear_relu_stack_4 : [num_users=1] = call_module[target=linear_relu_stack.4](args = (%linear_relu_stack_3,), kwargs = {})\n",
            "    %linear_relu_stack_5 : [num_users=1] = call_module[target=linear_relu_stack.5](args = (%linear_relu_stack_4,), kwargs = {})\n",
            "    return linear_relu_stack_5\n",
            "Symbolic Traced Code:\n",
            "\n",
            "\n",
            "def forward(self, x):\n",
            "    linear_relu_stack_0 = getattr(self.linear_relu_stack, \"0\")(x);  x = None\n",
            "    linear_relu_stack_1 = getattr(self.linear_relu_stack, \"1\")(linear_relu_stack_0);  linear_relu_stack_0 = None\n",
            "    linear_relu_stack_2 = getattr(self.linear_relu_stack, \"2\")(linear_relu_stack_1);  linear_relu_stack_1 = None\n",
            "    linear_relu_stack_3 = getattr(self.linear_relu_stack, \"3\")(linear_relu_stack_2);  linear_relu_stack_2 = None\n",
            "    linear_relu_stack_4 = getattr(self.linear_relu_stack, \"4\")(linear_relu_stack_3);  linear_relu_stack_3 = None\n",
            "    linear_relu_stack_5 = getattr(self.linear_relu_stack, \"5\")(linear_relu_stack_4);  linear_relu_stack_4 = None\n",
            "    return linear_relu_stack_5\n",
            "    \n",
            "Output before modification:tensor([[0.0639, 0.0515, 0.0795, 0.0000, 0.0219, 0.0886, 0.0142, 0.0000, 0.0258,\n",
            "         0.0000]], grad_fn=<ReluBackward0>)\n",
            "modified ReLU to Sigmoid\n",
            "modified ReLU to Sigmoid\n",
            "modified ReLU to Sigmoid\n",
            "Traced Modified Graph:graph():\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %linear_relu_stack_0 : [num_users=1] = call_module[target=linear_relu_stack.0](args = (%x,), kwargs = {})\n",
            "    %linear_relu_stack_1 : [num_users=1] = call_module[target=linear_relu_stack.1](args = (%linear_relu_stack_0,), kwargs = {})\n",
            "    %linear_relu_stack_2 : [num_users=1] = call_module[target=linear_relu_stack.2](args = (%linear_relu_stack_1,), kwargs = {})\n",
            "    %linear_relu_stack_3 : [num_users=1] = call_module[target=linear_relu_stack.3](args = (%linear_relu_stack_2,), kwargs = {})\n",
            "    %linear_relu_stack_4 : [num_users=1] = call_module[target=linear_relu_stack.4](args = (%linear_relu_stack_3,), kwargs = {})\n",
            "    %linear_relu_stack_5 : [num_users=1] = call_module[target=linear_relu_stack.5](args = (%linear_relu_stack_4,), kwargs = {})\n",
            "    return linear_relu_stack_5\n",
            "Output after modification:tensor([[0.5773, 0.5076, 0.5722, 0.4225, 0.5452, 0.6083, 0.6144, 0.3237, 0.4686,\n",
            "         0.3624]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    }
  ]
}